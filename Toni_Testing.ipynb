{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTO DO LIST:\\n    - evaluate the default algorithm\\n        - holdout\\n        - confusion matrix\\n        - ROC / AUC\\n    \\n    - compare the default algorithm with the modified ones for single task (my dataset)\\n        - get each modified algorithm houldout accuracy and standard deviation (SD) estimates\\n        - calculate the difference in accuracy and SD between the default model and each of the mdified ones\\n        - Are the differences statistically significant? -> Run hypothesis tests (paired t-test or wilcoxon)\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TO DO LIST:\n",
    "    - evaluate the default algorithm\n",
    "        - holdout\n",
    "        - confusion matrix\n",
    "        - ROC / AUC\n",
    "    \n",
    "    - compare the default algorithm with the modified ones for single task (my dataset)\n",
    "        - get each modified algorithm houldout accuracy and standard deviation (SD) estimates\n",
    "        - calculate the difference in accuracy and SD between the default model and each of the mdified ones\n",
    "        - Are the differences statistically significant? -> Run hypothesis tests (paired t-test or wilcoxon)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openml\n",
    "from AdaBoostWorkyWorky import AdaBoost\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc</th>\n",
       "      <th>v(g)</th>\n",
       "      <th>ev(g)</th>\n",
       "      <th>iv(g)</th>\n",
       "      <th>n</th>\n",
       "      <th>v</th>\n",
       "      <th>l</th>\n",
       "      <th>d</th>\n",
       "      <th>i</th>\n",
       "      <th>e</th>\n",
       "      <th>...</th>\n",
       "      <th>lOCode</th>\n",
       "      <th>lOComment</th>\n",
       "      <th>lOBlank</th>\n",
       "      <th>locCodeAndComment</th>\n",
       "      <th>uniq_Op</th>\n",
       "      <th>uniq_Opnd</th>\n",
       "      <th>total_Op</th>\n",
       "      <th>total_Opnd</th>\n",
       "      <th>branchCount</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.30</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>72.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>1134.130005</td>\n",
       "      <td>0.05</td>\n",
       "      <td>20.309999</td>\n",
       "      <td>55.849998</td>\n",
       "      <td>23029.099609</td>\n",
       "      <td>...</td>\n",
       "      <td>51.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>190.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>4348.759766</td>\n",
       "      <td>0.06</td>\n",
       "      <td>17.059999</td>\n",
       "      <td>254.869995</td>\n",
       "      <td>74202.671875</td>\n",
       "      <td>...</td>\n",
       "      <td>129.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>329.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>599.119995</td>\n",
       "      <td>0.06</td>\n",
       "      <td>17.190001</td>\n",
       "      <td>34.860001</td>\n",
       "      <td>10297.299805</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10880</th>\n",
       "      <td>18.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>241.479996</td>\n",
       "      <td>0.14</td>\n",
       "      <td>7.330000</td>\n",
       "      <td>32.930000</td>\n",
       "      <td>1770.859985</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10881</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>129.660004</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>15.720000</td>\n",
       "      <td>1069.680054</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10882</th>\n",
       "      <td>42.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>519.570007</td>\n",
       "      <td>0.04</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>19.680000</td>\n",
       "      <td>13716.719727</td>\n",
       "      <td>...</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10883</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>147.149994</td>\n",
       "      <td>0.12</td>\n",
       "      <td>8.440000</td>\n",
       "      <td>17.440001</td>\n",
       "      <td>1241.569946</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10884</th>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>272.630005</td>\n",
       "      <td>0.09</td>\n",
       "      <td>11.570000</td>\n",
       "      <td>23.559999</td>\n",
       "      <td>3154.669922</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10880 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         loc  v(g)  ev(g)  iv(g)      n            v     l          d  \\\n",
       "0        1.1   1.4    1.4    1.4    1.3     1.300000  1.30   1.300000   \n",
       "1        1.0   1.0    1.0    1.0    1.0     1.000000  1.00   1.000000   \n",
       "2       72.0   7.0    1.0    6.0  198.0  1134.130005  0.05  20.309999   \n",
       "3      190.0   3.0    1.0    3.0  600.0  4348.759766  0.06  17.059999   \n",
       "4       37.0   4.0    1.0    4.0  126.0   599.119995  0.06  17.190001   \n",
       "...      ...   ...    ...    ...    ...          ...   ...        ...   \n",
       "10880   18.0   4.0    1.0    4.0   52.0   241.479996  0.14   7.330000   \n",
       "10881    9.0   2.0    1.0    2.0   30.0   129.660004  0.12   8.250000   \n",
       "10882   42.0   4.0    1.0    2.0  103.0   519.570007  0.04  26.400000   \n",
       "10883   10.0   1.0    1.0    1.0   36.0   147.149994  0.12   8.440000   \n",
       "10884   19.0   3.0    1.0    1.0   58.0   272.630005  0.09  11.570000   \n",
       "\n",
       "                i             e  ...  lOCode  lOComment  lOBlank  \\\n",
       "0        1.300000      1.300000  ...     2.0        2.0      2.0   \n",
       "1        1.000000      1.000000  ...     1.0        1.0      1.0   \n",
       "2       55.849998  23029.099609  ...    51.0       10.0      8.0   \n",
       "3      254.869995  74202.671875  ...   129.0       29.0     28.0   \n",
       "4       34.860001  10297.299805  ...    28.0        1.0      6.0   \n",
       "...           ...           ...  ...     ...        ...      ...   \n",
       "10880   32.930000   1770.859985  ...    13.0        0.0      2.0   \n",
       "10881   15.720000   1069.680054  ...     5.0        0.0      2.0   \n",
       "10882   19.680000  13716.719727  ...    29.0        1.0     10.0   \n",
       "10883   17.440001   1241.569946  ...     6.0        0.0      2.0   \n",
       "10884   23.559999   3154.669922  ...    13.0        0.0      2.0   \n",
       "\n",
       "       locCodeAndComment  uniq_Op  uniq_Opnd  total_Op  total_Opnd  \\\n",
       "0                    2.0      1.2        1.2       1.2         1.2   \n",
       "1                    1.0      1.0        1.0       1.0         1.0   \n",
       "2                    1.0     17.0       36.0     112.0        86.0   \n",
       "3                    2.0     17.0      135.0     329.0       271.0   \n",
       "4                    0.0     11.0       16.0      76.0        50.0   \n",
       "...                  ...      ...        ...       ...         ...   \n",
       "10880                0.0     10.0       15.0      30.0        22.0   \n",
       "10881                0.0     12.0        8.0      19.0        11.0   \n",
       "10882                0.0     18.0       15.0      59.0        44.0   \n",
       "10883                0.0      9.0        8.0      21.0        15.0   \n",
       "10884                1.0     12.0       14.0      31.0        27.0   \n",
       "\n",
       "       branchCount  target  \n",
       "0              1.4      -1  \n",
       "1              1.0       1  \n",
       "2             13.0       1  \n",
       "3              5.0       1  \n",
       "4              7.0       1  \n",
       "...            ...     ...  \n",
       "10880          7.0      -1  \n",
       "10881          3.0      -1  \n",
       "10882          7.0      -1  \n",
       "10883          1.0      -1  \n",
       "10884          5.0      -1  \n",
       "\n",
       "[10880 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the data from OpenML\n",
    "# dataset from Task ID = 3904\n",
    "\n",
    "suite = openml.study.get_suite(99)\n",
    "task_id = 3904\n",
    "task = openml.tasks.get_task(task_id)    \n",
    "dataset = openml.datasets.get_dataset(task.dataset_id)\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "    dataset_format=\"array\", target=dataset.default_target_attribute\n",
    ")\n",
    "\n",
    "# Visualizar o dataset\n",
    "\n",
    "df = pd.DataFrame(X, columns=attribute_names)\n",
    "# converter:\n",
    "#   0 -> -1\n",
    "#   1 -> 1\n",
    "df['target'] = 2*y-1 \n",
    "df = df.dropna(how='any', axis=0)\n",
    "\n",
    "X = df.drop(columns=['target'], axis=1)\n",
    "y = df.target\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holdout estimation function\n",
    "\n",
    "def holdout_estimation(model, alpha, n_classifiers, x, y, test_size_value=0.3, seed=1111):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size_value, random_state=seed)\n",
    "    model.fit(x_train, y_train, alpha, n_classifiers)\n",
    "    y_pred = model.predict(x_test)\n",
    "    #print(f\"Estimated accuracy by holdout {accuracy_score(y_test, y_pred):.5f}, using {model.__class__.__name__}\")\n",
    "    return accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating the best n for alpha = 0\n",
      "Calculating the best n for alpha = 1\n",
      "Calculating the best n for alpha = 2\n",
      "{0: (50, 0.8183210784313726), 1: (140, 0.8186274509803921), 2: (120, 0.8131127450980392)}\n"
     ]
    }
   ],
   "source": [
    "# get the best number of classifiers in the AdaBoost for each alpha type of calc\n",
    "best_n_interators = {}\n",
    "for alpha in range(3):\n",
    "    print(\"Calculating the best n for alpha =\", alpha)\n",
    "    best_n_interators[alpha] = (0, 0)\n",
    "    for n in range(50, 151, 10):\n",
    "        ab = AdaBoost()\n",
    "        accuracy = holdout_estimation(ab, alpha, n, X, y)\n",
    "        if (accuracy > best_n_interators[alpha][1]):\n",
    "                best_n_interators[alpha] = (n, accuracy)\n",
    "\n",
    "print(best_n_interators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha calc method = 0\n",
      "Alpha calc method = 1\n",
      "Alpha calc method = 2\n",
      "0: [0.8177083333333334, 0.8106617647058824, 0.8128063725490197, 0.8152573529411765, 0.8152573529411765, 0.8164828431372549, 0.8167892156862745, 0.8164828431372549, 0.8152573529411765, 0.8134191176470589, 0.8161764705882353, 0.805453431372549, 0.8106617647058824, 0.8174019607843137, 0.8164828431372549, 0.8146446078431373, 0.8149509803921569, 0.8164828431372549, 0.8118872549019608, 0.8155637254901961, 0.8152573529411765, 0.8121936274509803, 0.8152573529411765, 0.8164828431372549, 0.8125, 0.8186274509803921, 0.8088235294117647, 0.8177083333333334, 0.8174019607843137, 0.8143382352941176, 0.8183210784313726, 0.8121936274509803, 0.8146446078431373, 0.8161764705882353, 0.8164828431372549, 0.8183210784313726, 0.8149509803921569, 0.8149509803921569, 0.8134191176470589, 0.8100490196078431, 0.8137254901960784, 0.8164828431372549, 0.8174019607843137, 0.8180147058823529, 0.8177083333333334, 0.8155637254901961, 0.8155637254901961, 0.8167892156862745, 0.8143382352941176, 0.8146446078431373]\n",
      "1: [0.8192401960784313, 0.8155637254901961, 0.819546568627451, 0.8183210784313726, 0.8177083333333334, 0.8183210784313726, 0.8174019607843137, 0.8177083333333334, 0.8158700980392157, 0.8158700980392157, 0.8164828431372549, 0.8152573529411765, 0.8158700980392157, 0.8167892156862745, 0.8158700980392157, 0.8158700980392157, 0.8149509803921569, 0.8164828431372549, 0.8170955882352942, 0.8207720588235294, 0.8164828431372549, 0.8149509803921569, 0.8161764705882353, 0.819546568627451, 0.8134191176470589, 0.8164828431372549, 0.8158700980392157, 0.8201593137254902, 0.8198529411764706, 0.8192401960784313, 0.8149509803921569, 0.8167892156862745, 0.8164828431372549, 0.8137254901960784, 0.8170955882352942, 0.8186274509803921, 0.8146446078431373, 0.8201593137254902, 0.8183210784313726, 0.8167892156862745, 0.8155637254901961, 0.8149509803921569, 0.8189338235294118, 0.8164828431372549, 0.8180147058823529, 0.8183210784313726, 0.8167892156862745, 0.8164828431372549, 0.8167892156862745, 0.8170955882352942]\n",
      "2: [0.7947303921568627, 0.796875, 0.8066789215686274, 0.8048406862745098, 0.7950367647058824, 0.8045343137254902, 0.7919730392156863, 0.8042279411764706, 0.7950367647058824, 0.7913602941176471, 0.7938112745098039, 0.7947303921568627, 0.7977941176470589, 0.8094362745098039, 0.7910539215686274, 0.7870710784313726, 0.8051470588235294, 0.7870710784313726, 0.7852328431372549, 0.7990196078431373, 0.7987132352941176, 0.7925857843137255, 0.8011642156862745, 0.7993259803921569, 0.7705269607843137, 0.805453431372549, 0.7996323529411765, 0.8057598039215687, 0.8045343137254902, 0.805453431372549, 0.8109681372549019, 0.7962622549019608, 0.7931985294117647, 0.8075980392156863, 0.8014705882352942, 0.7910539215686274, 0.7996323529411765, 0.8057598039215687, 0.8026960784313726, 0.8011642156862745, 0.7981004901960784, 0.8085171568627451, 0.8060661764705882, 0.7959558823529411, 0.7536764705882353, 0.8005514705882353, 0.8051470588235294, 0.8094362745098039, 0.8131127450980392, 0.7892156862745098]\n"
     ]
    }
   ],
   "source": [
    "# getting the accuracy results by holdout for 50 runs on the default and modified models\n",
    "\n",
    "scores = {}\n",
    "for alpha in range(3):\n",
    "    scores[alpha] = []\n",
    "    print(\"Alpha calc method =\", alpha)\n",
    "    for _ in range(50):\n",
    "        ab = AdaBoost()\n",
    "        scores[alpha].append(holdout_estimation(ab, alpha, best_n_interators[alpha][0], X, y))\n",
    "\n",
    "for key in scores.keys():\n",
    "    print(f\"{key}: {scores.get(key)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averages for each method to calculate the alpha:\n",
      "\t {0: 0.8149632352941175, 1: 0.8170036764705884, 2: 0.7981678921568629}\n",
      "\n",
      "Standard deviation for each method to calculate the alpha:\n",
      "\t {0: 0.0026276407416110504, 1: 0.0016994703735733632, 2: 0.010066245771566638}\n"
     ]
    }
   ],
   "source": [
    "# check statistics for each model\n",
    "\n",
    "averages = {}\n",
    "stddev = {}\n",
    "for alpha in range(3):\n",
    "    averages[alpha] = np.mean(scores[alpha])\n",
    "    stddev[alpha] = np.std(scores[alpha])\n",
    "\n",
    "print(\"Averages for each method to calculate the alpha:\\n\\t\", averages)\n",
    "print()\n",
    "print(\"Standard deviation for each method to calculate the alpha:\\n\\t\", stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of difference \t Standard Deviation of difference\n",
      "-0.0020404411764705934  \t 0.0026522804458316714\n",
      "0.016795343137254913  \t 0.01010930810699967\n",
      "0.01883578431372551  \t 0.009950127643844408\n"
     ]
    }
   ],
   "source": [
    "# differences between the results\n",
    "\n",
    "default_scores = np.array(scores[0])\n",
    "alpha1_scores = np.array(scores[1])\n",
    "alpha2_scores = np.array(scores[2])\n",
    "\n",
    "d_a1 = default_scores - alpha1_scores\n",
    "d_a2 = default_scores - alpha2_scores\n",
    "a1_a2 = alpha1_scores - alpha2_scores\n",
    "\n",
    "print(\"Average of difference \\t Standard Deviation of difference\")\n",
    "print(np.mean(d_a1), \" \\t\", np.std(d_a1))\n",
    "print(np.mean(d_a2), \" \\t\", np.std(d_a2))\n",
    "print(np.mean(a1_a2), \" \\t\", np.std(a1_a2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value for Default - Alpha1: 2.0393834070912178e-06\n",
      "p-value for Default - Alpha2: 1.0620902274728687e-15\n",
      "p-value for Alpha1 - Alpha2: 8.060108727564165e-18\n"
     ]
    }
   ],
   "source": [
    "# compare models using paired t-test\n",
    "\n",
    "t_test_d_a1 = ss.ttest_rel(default_scores, alpha1_scores) # --> returns tuple (statistic, p-value, df) --> we are mainly interested in the p-value\n",
    "t_test_d_a2 = ss.ttest_rel(default_scores, alpha2_scores)\n",
    "t_test_a1_a2 = ss.ttest_rel(alpha1_scores, alpha2_scores)\n",
    "\n",
    "print(\"p-value for Default - Alpha1:\", t_test_d_a1[1])\n",
    "print(\"p-value for Default - Alpha2:\", t_test_d_a2[1])\n",
    "print(\"p-value for Alpha1 - Alpha2:\", t_test_a1_a2[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value for Defalut - Alpha1: 5.7075242891466285e-06\n",
      "p-value for Defalut - Alpha2: 1.7763568394002505e-15\n",
      "p-value for Alpha1 - Alpha2: 1.7763568394002505e-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\scipy\\stats\\_morestats.py:3414: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n"
     ]
    }
   ],
   "source": [
    "# compare models using wilcoxon signed rank test\n",
    "\n",
    "wilcox_d_a1 = ss.wilcoxon(default_scores, alpha1_scores) # --> returns tuple (statistic, p-value) --> we are mainly interested in the p-value\n",
    "wilcox_d_a2 = ss.wilcoxon(default_scores, alpha2_scores)\n",
    "wilcox_a1_a2 = ss.wilcoxon(alpha1_scores, alpha2_scores)\n",
    "\n",
    "print(\"p-value for Defalut - Alpha1:\", wilcox_d_a1[1])\n",
    "print(\"p-value for Defalut - Alpha2:\", wilcox_d_a2[1])\n",
    "print(\"p-value for Alpha1 - Alpha2:\", wilcox_a1_a2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m models \u001b[39m=\u001b[39m [\n\u001b[1;32m----> 2\u001b[0m     AdaBoost(\u001b[39m0\u001b[39;49m),\n\u001b[0;32m      3\u001b[0m     AdaBoost(\u001b[39m1\u001b[39m),\n\u001b[0;32m      4\u001b[0m     AdaBoost(\u001b[39m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m ]\n\u001b[0;32m      7\u001b[0m models\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    AdaBoost(0),\n",
    "    AdaBoost(1),\n",
    "    AdaBoost(2)\n",
    "]\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cross-validation on an algorithm\n",
    "\n",
    "def run_cv(X,y,algs,nfolds=10):\n",
    "    results = {}\n",
    "    kf = KFold(n_splits=nfolds, shuffle=True, random_state=1111)\n",
    "    for algo_name, algo in algs:\n",
    "        results[algo_name] = []\n",
    "        for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            algo.fit(X_train, y_train)\n",
    "            y_pred = algo.predict(X_test)\n",
    "            results[algo_name].append(accuracy_score(y_test, y_pred))\n",
    "    results_df = pd.DataFrame.from_dict(results)\n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results using a box plot\n",
    "\n",
    "def plot_cv(results_cv,metric='Accuracy'):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(results_cv)\n",
    "    ax.set_xticklabels(results_cv.columns)\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(\"Cross-validation results for multiple algorithms in a single task\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results using a bar plot with error bars\n",
    "\n",
    "def plot_cv_estimates(results_cv,metric='Accuracy'):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(results_cv.columns, results_cv.mean(), yerr=results_cv.std() / np.sqrt(results_cv.shape[0]), capsize=5)\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(\"Cross-validation results for multiple algorithms in a single task\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
