{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Notebook - Machine Learning I"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openml\n",
    "from AdaBoostClassifier import AdaBoost\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "import scipy.stats as ss\n",
    "import scikit_posthocs as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the datasets to use throughout the project\n",
    "\n",
    "Function to read a dataset from [OpenML-CC18 Curated Classification benchmark](https://www.openml.org/search?type=study&sort=tasks_included&study_type=task&id=99 \"OpenML-CC18\") givent its Task ID and return the corresponding Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(task_id):\n",
    "    suite = openml.study.get_suite(99)\n",
    "    task = openml.tasks.get_task(task_id)    \n",
    "    dataset = openml.datasets.get_dataset(task.dataset_id)\n",
    "\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"array\", target=dataset.default_target_attribute\n",
    "    )\n",
    "\n",
    "    df = pd.DataFrame(X, columns=attribute_names)\n",
    "    # convert:\n",
    "    #   0 -> -1\n",
    "    #   1 -> 1\n",
    "    df['target'] = 2*y-1 \n",
    "    # erase rows with NaN values\n",
    "    df = df.dropna(how='any', axis=0)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the datasets used in this project:\n",
    "- Dataset about breast cancer, with 683 entries;\n",
    "- Dataset about mushrooms, with 5644 entries;\n",
    "- Dataset made with synthetic data, with 10880 entries;\n",
    "- Dataset about the Wilt decease on plants, with 3279 entries;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    (\"Task ID = 15\", getDataset(15)),\n",
    "    (\"Task ID = 24\", getDataset(24)),\n",
    "    (\"Task ID = 3904\", getDataset(3904)),\n",
    "    (\"Task ID = 146820\", getDataset(146820))\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance evaluation of AdaBoost\n",
    "\n",
    "To get a better understanding of how AdaBoost behaves with the chosen datasets,, we implemented the function `run_cv()` to be able to perform 10-fold Cross Validation on our set of algoritms, obtaining an accuracy value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(X,y,algs,nfolds=10, means_only=False):\n",
    "    results = {}\n",
    "    kf = KFold(n_splits=nfolds, shuffle=True, random_state=1111)\n",
    "    for algo_name, algo in algs:\n",
    "        results[algo_name] = []\n",
    "        for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
    "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            algo.fit(X_train, y_train, M=40)\n",
    "            y_pred = algo.predict(X_test)\n",
    "            results[algo_name].append(accuracy_score(y_test, y_pred))\n",
    "    results_df = pd.DataFrame.from_dict(results)\n",
    "    if not means_only:\n",
    "        return results_df\n",
    "    else:\n",
    "        results_means = {}\n",
    "        for algo_name, algo in algs:\n",
    "            results_means[algo_name] = [np.mean(results[algo_name])]\n",
    "        return pd.DataFrame.from_dict(results_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_id, ds in datasets:\n",
    "    X = ds.drop(columns=['target'], axis=1)\n",
    "    y = ds['target']\n",
    "\n",
    "    algs = [(\"AdaBoost\", AdaBoost())]\n",
    "    result = run_cv(X, y, algs)\n",
    "    print(f\"Mean accuracy for dataset with {ds_id} in 10-fold CV: {np.mean(result['AdaBoost']):.5f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also decided to implement `plot_cm()` in order to attain a confusion matrix for each of the fitted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(model_fit,X_test,y_test):\n",
    "    y_pred = model_fit.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_id, ds in datasets:\n",
    "    X = ds.drop(columns=['target'], axis=1)\n",
    "    y = ds['target']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1111)\n",
    "    ab = AdaBoost()\n",
    "    ab.fit(X_train, y_train)\n",
    "    plot_cm(ab, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We figured we could take another look at the algorithm's behaviour by checking how it learns when raising and lowering the train size.  \n",
    "`plot_learning_curves()` was used to plot the Learning Curve of the fitted model. We ran 10 tests with training size = `tr_size` and stored all accuracy results in a line of train_scores and test_scores.   \n",
    "Finally, we were able to get the mean training score and the standard deviation for each train size and plot the learning curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(model, X, y, title=''):\n",
    "    training_size = np.linspace(0.1,0.9,10)\n",
    "    train_scores = {}\n",
    "    test_scores = {}\n",
    "    for tr_size in training_size:\n",
    "        train_scores[tr_size] = []\n",
    "        test_scores[tr_size] = []\n",
    "        for _ in range(10):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=tr_size)\n",
    "            model.fit(X_train, y_train)\n",
    "            training_pred = model.predict(X_train)\n",
    "            testing_pred = model.predict(X_test)\n",
    "            train_scores[tr_size].append(accuracy_score(training_pred, y_train))\n",
    "            test_scores[tr_size].append(accuracy_score(testing_pred, y_test))\n",
    "        print(\n",
    "            f\"Train size: {tr_size}\\n \\\n",
    "            Mean training score: {round(np.mean(train_scores[tr_size]), 5)}\\n \\\n",
    "            Standard deviation: {round(np.std(train_scores[tr_size]), 5)}\"\n",
    "        )\n",
    "\n",
    "    train_mean = np.array([np.mean(train_scores[tr_size]) for tr_size in training_size])\n",
    "    train_std = np.array([np.std(train_scores[tr_size]) for tr_size in training_size])\n",
    "    test_mean = np.array([np.mean(test_scores[tr_size]) for tr_size in training_size])\n",
    "    test_std = np.array([np.std(test_scores[tr_size]) for tr_size in training_size])\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(training_size, train_mean, 'o-', color='blue', label='Training score')\n",
    "    plt.plot(training_size, test_mean, 'o-', color='green', label='Test score')\n",
    "    plt.fill_between(training_size, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    plt.fill_between(training_size, test_mean - test_std, test_mean + test_std, alpha=0.1, color='green')\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Learning Curves '+title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_id, ds in datasets:\n",
    "    X = ds.drop(columns=['target'], axis=1)\n",
    "    y = ds['target']\n",
    "\n",
    "    ab = AdaBoost()\n",
    "    plot_learning_curves(ab, X, y, title=\"AdaBoost\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ROC Curve plots the true positive rate (sensitivity) against the false positive rate (1-specificity) for various threshold values, it allows us to, once again, have a visual interpretation of AdaBoost's performance.  \n",
    "Due to its robustness to class imbalance and threshold-independancy, we also believed the AUC Score to be a good measure of how good our chosen classifier was.  \n",
    "We used `plot_roc_curve()` to acquire both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_pred):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_id, ds in datasets:\n",
    "    ada_boost = AdaBoost()\n",
    "    X = ds.drop(columns=['target'], axis=1)\n",
    "    y = ds['target']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1111)\n",
    "    ada_boost.fit(X_train, y_train)\n",
    "    y_pred = ada_boost.predict(X_test)\n",
    "    plot_roc_curve(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of base algorithm and modified ones\n",
    "\n",
    "Definition of the models:\n",
    "- `default` is the default AdaBoost classifier.\n",
    "- `Alpha Changed` is the AdaBoost classifier with a different method to calculate alpha, the value used when updating the sample weights. In this model, the alpha value is directly proportional to the error - in other words, alpha == error.\n",
    "- `Duplicate Misclassified` is the AdaBoost classifier that, at each iteration, duplicates misclassified samples with a given probability P. P is calculated by dividing the number of new misclassified samples by the total size of samples (including previously added ones).\n",
    "- `Alpha Changed & Duplicate Misclassified` combines both of the two previous methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    (\"default\", AdaBoost()),\n",
    "    (\"Alpha Changed\", AdaBoost(alpha_type=1)),\n",
    "    (\"Duplicate Misclassified\", AdaBoost(duplicate=True)),\n",
    "    (\"Alpha Changed & Duplicate Misclassified\", AdaBoost(alpha_type=1, duplicate=True))\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`plot_cv` plots the results from 10-fold Cross Validation for the 4 used algorithms. We applied this function to each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cv(results_cv,metric='Accuracy', title=\"Cross-validation results for multiple algorithms in a single task\"):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(results_cv)\n",
    "    ax.set_xticklabels(results_cv.columns)\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds_id, ds in datasets:\n",
    "    X = ds.drop(columns=['target'], axis=1)\n",
    "    y = ds['target']\n",
    "    results = run_cv(X, y, models)\n",
    "    plot_cv(results, title=f\"Cross-validation for the algorithms in the dataset with {ds_id}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a better understanding of how effective our changes were, it's possible to see in a table below the accuracy obtained from all of the datasets with all of the new models (using the previously mentioned `run_cv` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv_per_dataset(datasets, algs):\n",
    "    table = pd.DataFrame()\n",
    "    for ds_id, ds in datasets:\n",
    "        X = ds.drop(columns=['target'], axis=1)\n",
    "        y = ds['target']\n",
    "        result_ds = run_cv(X, y, algs, means_only=True)\n",
    "        table = pd.concat([table, result_ds], ignore_index=True)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = run_cv_per_dataset(datasets, models)\n",
    "table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also possible to analyse their performance compared to each other by ranking the models. Below there's another table where each line (0-3) is a dataset and the respective collumn shows the models' position in ranking.  \n",
    "Line 4 holds the average rank for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the rank method to get the ranking of classifiers per data set\n",
    "def get_ranks(estimates_df,asc=False):\n",
    "    results_rank = estimates_df.rank(axis=1, ascending=asc, method='min')\n",
    "    results_rank.loc[len(results_rank)] = results_rank.mean()\n",
    "    return results_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ranks = get_ranks(table)\n",
    "table_ranks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we ran a Friedman test to check whether all of the models could be considered equivalent (and had equal rankings across the tasks).  \n",
    "Considering that the p-value is greater than alpha, we can accept the hypothesis of the algorithms being equivalent with a significancy level of 5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_comp(estimates_df,alpha=0.05):\n",
    "    fvalue, pvalue = ss.friedmanchisquare(*[estimates_df[alg] for alg in estimates_df])\n",
    "    print(f'p-value = {pvalue:.3f}')\n",
    "    if pvalue >= alpha:\n",
    "        print(f'There is no significant difference in performance among the algorithms')\n",
    "        return None\n",
    "\n",
    "    print(f'There is a significant difference in performance among the algorithms')\n",
    "    pairedcomp = sp.posthoc_nemenyi_friedman(estimates_df)\n",
    "    return(pairedcomp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_comparision = multiple_comp(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
